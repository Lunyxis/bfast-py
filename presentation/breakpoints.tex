\documentclass[presentation.tex]{subfiles}

\begin{document}

\begin{frame}
\frametitle{Breakpoint Estimation - Intro}
\begin{itemize}
  \item Described in the paper by Bai and Perron from 2003
  \item Estimate the number and position of breakpoints in a time series using
    a dynamic programming algorithm and Bayesian Information Criterion (BIC)
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Breakpoint Estimation - The Model}
\begin{itemize}
  \item 
We assume a pure structural change model for $m$ breaks ($m+1$ segments):
\[
y_{t} = x_{t}^{\top} \beta_{j}+u_{t} \quad t=T_{j-1}+1, \ldots, T_{j}
\]
for $j = 1,\hdots , m+1$, and where:
\vspace{2.5mm}
\begin{itemize}
\item $x_t \in \mathbb{R}^q$ is the value of the independent variable at time
  $t = 1,\hdots , T$
\item $y_{t} \in \mathbb{R}$ is the observation at time $t$
\item $\beta_j: \; (j=1, \hdots ,m+1)$ is the vector of coefficients for the segment $j$
\item $u_{t} \in \mathbb{R}$ is the disturbance (error) at time $t$
\item $(T_1, \hdots ,T_m)$ are the unknown indices of the $m$ breakpoints.
 We additionally set $T_0 = 0$ and $T_{m+1} = T$
\end{itemize}
\item 
In other words, we split the time series into $m+1$ segments (of potentially
different sizes) and perform linear
regression independently for each segment, where for segment $i$, we estimate
a coefficient vector $\beta_i$.
Hence, we find the estimate of the coefficient vector ($\hat{\beta}$) for each
m-partition $(T_1, ..., T_m)$ by minimizing the sum of squared residuals:
\[
\sum_{i=1}^{m+1}\sum_{i=T_{i-1} + 1}^{T_i} \left[ y_t - x_t^{\top}\beta_i \right]^2
\]
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{}
\begin{itemize}
\item
Let $\{T_j\}$ denote an m-partition $(T_1, ..., T_m)$ and $S_T(T_1, ..., T_m)$
denote the resulting sum of squared residuals. Since we can estimate the
coefficient vector for each partition, we can minimize the sum of
squared residuals by finding the optimal position for the breakpoints
\[
(\hat{T}_1, ..., \hat{T}_m) = \operatorname{argmin}_{T_{1}, \ldots, T_{m}}
S_{T}\left(T_{1}, \ldots, T_{m}\right)
\]
\item
There is a finite number of possible breakpoints, and only a subset of
possible partitions is feasible. There are $T(T+1)/2$ (sum of integers from 1 to T)
possible segments that can be chosen. This can be demonstrated by building a
matrix of possible segments, with starting date on the y-axis and terminal date
on the y-axis. A length of a segment is positive, hence we can eliminate
one half of the potential segments. There are further reductions
that can be made that reduce the number of feasible segments to
$T(T+1)/2 - \left(T(h-1)-m h(h-1)-(h-1)^{2}-h(h-1) / 2\right)$, where $h$ is the minimal
segment length.
\end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Upper-diagonal Matrix of Sums of Squared Residuals}
\begin{table}[]
\begin{tabular}{lllllllllll}
                                   & \multicolumn{10}{c}{stop}                                                                                                                                                                                                                                                 \\ \cline{2-11} 
\multicolumn{1}{l|}{}              & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{1}    & \multicolumn{1}{l|}{2}    & \multicolumn{1}{l|}{3}    & \multicolumn{1}{l|}{4}    & \multicolumn{1}{l|}{5}    & \multicolumn{1}{l|}{6}    & \multicolumn{1}{l|}{7}    & \multicolumn{1}{l|}{8}    & \multicolumn{1}{l|}{9}    \\ \cline{2-11} 
\multicolumn{1}{l|}{}              & \multicolumn{1}{l|}{1} & \multicolumn{1}{l|}{$\operatorname{nf}_1$} & \multicolumn{1}{l|}{$\operatorname{nf}_1$} & \multicolumn{1}{l|}{$\operatorname{nf}_1$} & \multicolumn{1}{l|}{f}   & \multicolumn{1}{l|}{f}   & \multicolumn{1}{l|}{f}   & \multicolumn{1}{l|}{$\operatorname{nf}_2$} & \multicolumn{1}{l|}{$\operatorname{nf}_2$} & \multicolumn{1}{l|}{$\operatorname{nf}_2$} \\ \cline{2-11} 
\multicolumn{1}{l|}{}              & \multicolumn{1}{l|}{2} & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{$\operatorname{nf}_1$} & \multicolumn{1}{l|}{$\operatorname{nf}_1$} & \multicolumn{1}{l|}{$\operatorname{nf}_1$} & \multicolumn{1}{l|}{$\operatorname{nf}_3$} & \multicolumn{1}{l|}{$\operatorname{nf}_3$} & \multicolumn{1}{l|}{$\operatorname{nf}_2$} & \multicolumn{1}{l|}{$\operatorname{nf}_2$} & \multicolumn{1}{l|}{$\operatorname{nf}_2$} \\ \cline{2-11} 
\multicolumn{1}{l|}{}              & \multicolumn{1}{l|}{3} & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{$\operatorname{nf}_1$} & \multicolumn{1}{l|}{$\operatorname{nf}_1$} & \multicolumn{1}{l|}{$\operatorname{nf}_1$} & \multicolumn{1}{l|}{$\operatorname{nf}_3$} & \multicolumn{1}{l|}{$\operatorname{nf}_2$} & \multicolumn{1}{l|}{$\operatorname{nf}_2$} & \multicolumn{1}{l|}{$\operatorname{nf}_2$} \\ \cline{2-11} 
\multicolumn{1}{l|}{start} & \multicolumn{1}{l|}{4} & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{$\operatorname{nf}_1$} & \multicolumn{1}{l|}{$\operatorname{nf}_1$} & \multicolumn{1}{l|}{$\operatorname{nf}_1$} & \multicolumn{1}{l|}{f}   & \multicolumn{1}{l|}{f}   & \multicolumn{1}{l|}{f}   \\ \cline{2-11} 
\multicolumn{1}{l|}{}              & \multicolumn{1}{l|}{5} & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{$\operatorname{nf}_1$} & \multicolumn{1}{l|}{$\operatorname{nf}_1$} & \multicolumn{1}{l|}{$\operatorname{nf}_1$} & \multicolumn{1}{l|}{f}   & \multicolumn{1}{l|}{f}   \\ \cline{2-11} 
\multicolumn{1}{l|}{}              & \multicolumn{1}{l|}{6} & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{$\operatorname{nf}_1$} & \multicolumn{1}{l|}{$\operatorname{nf}_1$} & \multicolumn{1}{l|}{$\operatorname{nf}_1$} & \multicolumn{1}{l|}{f}   \\ \cline{2-11} 
\multicolumn{1}{l|}{}              & \multicolumn{1}{l|}{7} & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{$\operatorname{nf}_1$} & \multicolumn{1}{l|}{$\operatorname{nf}_1$} & \multicolumn{1}{l|}{$\operatorname{nf}_1$} \\ \cline{2-11} 
\multicolumn{1}{l|}{}              & \multicolumn{1}{l|}{8} & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{$\operatorname{nf}_1$} & \multicolumn{1}{l|}{$\operatorname{nf}_1$} \\ \cline{2-11} 
\multicolumn{1}{l|}{}              & \multicolumn{1}{l|}{9} & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{}     & \multicolumn{1}{l|}{$\operatorname{nf}_1$} \\ \cline{2-11} 
\end{tabular}
\caption{An example of an upper-triangular matrix of sums of squared residuals for $T =
  9$, $h=3$, $m=2$. We must compute sum of squared residuals for all feasible
  segments (f).
}
\end{table}
\end{frame}


\begin{frame}
  \frametitle{Recursive Residuals}
  \begin{itemize}
  \item 
In order to calculate the sum of squared residuals for each feasible segment in
the upper-triangular matrix, me must use the recursive residuals of a linear
regression model. Let us assume a simple regression model with no breakpoints
\[
y_i = x_i^{\top} \beta + u_i \quad i = (1, \hdots, n)
\]
\[
Y = \mathrm{X}\beta + U
\]
where $Y, U$ are $n \times 1$ vectors, $\mathrm{X}$ is a $(n \times k)$ matrix
and $\beta$ is $k \times 1$ vector.

\item 
Let $\hat{\beta}^{(i)}$ be the OLS-estimate of $\beta$
based on all observations up to $i$, and $X^{(i)}$ be the subset of the
$\mathrm{X}$ with rows from $k$ to $i$.
Then the recursive residuals can be calculated:
\[
v(i)=\frac{y_{i}-x_{i}^{\top}
  \hat{\beta}^{(i-1)}}{\sqrt{1+x_{i}^{\top}\left(X^{(i-1)^{\top}}
    X^{(i-1)}\right)^{-1} x_{i}}} \quad(i=k, \ldots, n)
\]

  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Calculating the SSR Table}
Since we are considering a pure
structural change model, there are no constraints between the segments and we
can therefore apply OLS for each segment independently.

Let $\text{SSR}(i, j)$  be the sum of squared residuals, acquired from
application of OLS to segment that starts at $i$ and ends at $j$.
It can be calculated using a recursive procedure:
\[
\text{SSR}(i,j) = \text{SSR}(i, j - 1) + v(i)^2
\]
We can calculate all row in the table in parallel, and each row can be calculated
in a following manner:
\begin{verbatim}
betas = map (\i -> OLS(X[k:i], y[k:i])) [k:n]
SSR_ROW(i) = scan (+) 0 (map2 (\j b -> v(j, b) ** 2) [k:i] betas)
\end{verbatim}
\end{frame}

\begin{frame}
  \frametitle{Dynamic Programming Algorithm}
  \begin{itemize}
  \item 
After the table of squared residuals is constructed, we can apply the dynamic
programming algorithm in order to find the optimal partition that solves the
following recursive problem:
\[
\operatorname{SSR}\left(\left\{T_{m, T}\right\}\right)=\min _{m h \leq j \leq
  T-h}\left[\operatorname{SSR}\left(\left\{T_{m-1,
    j}\right\}\right)+\operatorname{SSR}(j+1, T)\right]
\]
where $m$ is the number of breakpoints, $T$ is the number of observations,
$h$ is the minimal segment length (distance between two breakpoints) and
$\operatorname{SSR}\left(\left\{T_{r, n}\right\}\right)$ be the sum of squared
residuals associated with the optimal partition that contains $r$ breaks, using
the first $n$ observations.
\item 
We can unroll the recursive calls in order to gain more insight into the workings of this method
and get a following expression:
\[
\begin{array}{c}
  S S R\left(\left\{T_{m, T}\right\}\right)= \\
  \min _{m h \leq j_{1} \leq T-h}\left[S S R\left(j_{1}+1, T\right)+\right. \\
  \min _{(m-1) h \leq j_{2} \leq j_{1}-h}\left[S S R\left(j_{2}+1, j_{1}\right)+\right. \\
  \vdots \\
  \left.\left.\left.\min _{h \leq j_{m} \leq j_{m-1}-h}
  \left[\operatorname{SSR}\left(1, j_{m}\right)+\operatorname{SSR}\left(j_{m}+1, j_{m-1}\right)\right]
  \ldots\right]\right]\right]
\end{array}
\]
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Dynamic Programming Algorithm - Continued}
  \begin{itemize}
  \item 
We start the procedure by solving the minimization problem from the
last line of the expression. We evaluate the optimal one-break partition for all
sub-samples that allow a possible break ranging from observations $h$ to
$T - mh$ (since we still have to fit $m$ segments of minimal size $h$ after
the first breakpoint). We then store $T - (m + 1)h + 1$ optimal breakpoints
along with the corresponding sums of squared residuals. These one-break
partitions would have an endpoint between $2h$ and $T - (m-1)h$. 
\item 
Then, we continue to 2-partitions that can have and endpoint between $3h$ and
$T - (m-2)h$. For each of $T - (m + 1)h + 1$ endpoints, we select an
1-partition that we have calculated earlier that would minimize the sum of
squared residuals. We then store the 2-partitions and corresponding sums of
squared residuals.
\item 
We iterate until we have computed the $T - (m + 1)h + 1$
$(m-1)$-partitions with endpoints between $(m-1)h$ and $T-2h$.
We finish by finding the optimal $(m-1)$ partition that gives the overall
minimal sum of squared residuals, when the last segment is appended to it.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Example}
  Consider this example. Let $T=20$, $h=4$ and $m=3$.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{imgs/breakpoints1}
  \caption{Allowed placements of $j_m$}
  \label{plt_4_1}
\end{figure}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{imgs/breakpoints2}
  \caption{Possible endpoints of 1-partion}
  \label{plt_4_2}
\end{figure}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{imgs/breakpoints3}
  \caption{Possible start points of the last segment}
  \label{plt_4_3}
\end{figure}
\end{frame}


\begin{frame}
  \frametitle{BIC}
 \begin{itemize}
 \item 
   For each possible number of breakpoints, up to some
   limit $m_{\text{max}}$, we find the sum of squared residuals for all breakpoints
   and then calculate the Bayesian Information Criterion:
   \[
   \text{BIC} = n_{\text{df}} \ln{n} - 2 {\hat{L}}
   \]
   where:
   \begin{itemize}
   \item $n_{\text{df}}$: the number of degrees of freedom for our model, in our case
     $n_{\text{df}} = (k + 1) (m + 1)$, where $k$ is the dimensionality of the vector $x_i$ and
     $m$ is the number of breakpoints.
   \item $n$ is the number of observations
   \item $\hat{L}$ is the maximized value of the log-likelihood of the model:
     \[
     \hat{L} = -\frac{n}{2} \left(\ln \left(S_T(\hat{T}_1,..,\hat{T}_m)\right)
     + \ln{\frac{2\pi}{n}} + 1\right)
     \]
     where $S_T(\hat{T}_1,..,\hat{T}_m)$ is the sum of squared residuals for the
     optimal m-partition of the time series (time series with m breakpoints). 
   \end{itemize}
 \item 
   The optimal number of breakpoints would be the number of breakpoints for which
   the value of BIC is lowest.
 \end{itemize} 
\end{frame}


\begin{frame}
  \frametitle{Algorithm Steps}
  \begin{enumerate}
\item \textbf{Calculate the upper-triangular matrix of sums of squared residuals}
\item \textbf{Find the largest number of allowed breakpoints}\\
  Given a value of $0\leq h' \leq 1$, which signifies the smallest allowed size of
  a segment, and $m_{\text{user}}$ which is the maximum number of breakpoints, determined by the user, 
  we can then find the largest number of allowed
  breakpoints $m_{\text{max}} = \min(\floor{\frac{1}{h'}} + 1, m_{\text{user}})$.
\item \textbf{Create a table \texttt{BIC} of size $m_{\text{max}} + 1$}
\item \textbf{For $m \in (m_{\text{max}}, ... 0)$, do following:}
  \begin{enumerate}
  \item Find the optimal position of the $m$ breakpoints, using the dynamic programming algorithm
  \item Calculate the sum of squared residuals, and then use it to
    calculate the Bayesian Information Criterion $\text{BIC}_m$ of this
    partition
  \item Add $\text{BIC}_m$ to the table
  \end{enumerate}
\item \textbf{Find the value of $m_{best}$ that corresponds to the minimal value in the \texttt{BIC}}
\item \textbf{Use the dynamic programming algorithm to find the $m_{best}$ breakpoints}
\end{enumerate}
\end{frame}

%% \begin{frame}
%%   \frametitle{Breakpoint Estimation - The Model Continued}
%%   \begin{itemize}
%%     \item 
%% This linear regression system can also be expressed in matrix form:
%% \[
%% Y = \overline{X}\beta + U
%% \]
%% where:
%% \[
%% Y =
%% \begin{bmatrix}
%% y_1 \\
%% y_2 \\
%% \vdots \\
%% y_T
%% \end{bmatrix}
%% \quad
%% \beta =
%% \begin{bmatrix}
%% \vert & \vert &  & \vert \\
%% \beta_{1} & \beta_{2}  & \hdots & \beta_{m+1}\\
%% \vert & \vert &  & \vert\\
%% \end{bmatrix}
%% \quad
%% U =
%% \begin{bmatrix}
%% u_1 \\
%% u_2 \\
%% \vdots \\
%% u_T
%% \end{bmatrix}
%% \]
%% and $\overline{X}$ is a block matrix that
%% diagonally partitions $X$ at breaking points $(T_1,..., T_m)$:
%% \[
%% \overline{X} =
%% \begin{bmatrix}
%% X_1 & 0 & \hdots & 0\\
%%  0 & X_2 & \hdots & 0 \\
%% \vdots & \vdots & \ddots & \vdots \\
%%  0 & 0 & \hdots & X_{m+1} 
%% \end{bmatrix}
%% \quad
%% X_i = 
%% \begin{bmatrix}
%%   \text{---} \hspace{-0.2cm} & x_{T_{i-1} + 1}^{\top} & \hspace{-0.2cm}\text{---} \\
%%   \text{---} \hspace{-0.2cm} & x_{T_{i-1} + 2}^{\top} & \hspace{-0.2cm}\text{---} \\
%%   & \vdots & \\ 
%%  \text{---} \hspace{-0.2cm} & x_{T_i}^{\top} & \hspace{-0.2cm}\text{---}  \\
%% \end{bmatrix}
%% \]
%% \end{itemize}  
%% \end{frame}

\end{document}
